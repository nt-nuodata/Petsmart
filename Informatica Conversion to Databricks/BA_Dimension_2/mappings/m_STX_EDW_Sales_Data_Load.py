# Databricks notebook source
# MAGIC %run "./udf_informatica"

# COMMAND ----------


from pyspark.sql.types import *

spark.sql("use DELTA_TRAINING")
spark.sql("set spark.sql.legacy.timeParserPolicy = LEGACY")


# COMMAND ----------
# DBTITLE 1, DW_LOAD_CONTROL_0


df_0=spark.sql("""
    SELECT
        DW_LOAD_CONTROL_DT AS DW_LOAD_CONTROL_DT,
        TXN_TSTMP AS TXN_TSTMP,
        TXN_KEY_GID AS TXN_KEY_GID,
        SITE_NBR AS SITE_NBR,
        REGISTER_NBR AS REGISTER_NBR,
        TXN_NBR AS TXN_NBR,
        DS_ORDER_NBR AS DS_ORDER_NBR,
        DS_ORDER_SEQ_NBR AS DS_ORDER_SEQ_NBR,
        TXN_TYPE_ID AS TXN_TYPE_ID,
        TXN_WAS_MID_VOIDED_FLAG AS TXN_WAS_MID_VOIDED_FLAG,
        TXN_WAS_POST_VOIDED_FLAG AS TXN_WAS_POST_VOIDED_FLAG,
        COUNTRY_CD AS COUNTRY_CD,
        LOCATION_ID AS LOCATION_ID,
        LOAD_TSTMP AS LOAD_TSTMP,
        DS_CHANNEL AS DS_CHANNEL,
        DS_ASSIST_SITE_NBR AS DS_ASSIST_SITE_NBR,
        DS_ASSIST_LOCATION_ID AS DS_ASSIST_LOCATION_ID,
        DS_CURRENCY_CD AS DS_CURRENCY_CD,
        monotonically_increasing_id() AS Monotonically_Increasing_Id 
    FROM
        DW_LOAD_CONTROL""")

df_0.createOrReplaceTempView("DW_LOAD_CONTROL_0")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_DW_LOAD_CONTROL1_1


df_1=spark.sql("""
    SELECT
        DW_LOAD_CONTROL_DT AS DW_LOAD_CONTROL_DT,
        TXN_TSTMP AS TXN_TSTMP,
        TXN_KEY_GID AS TXN_KEY_GID,
        SITE_NBR AS SITE_NBR,
        REGISTER_NBR AS REGISTER_NBR,
        TXN_NBR AS TXN_NBR,
        DS_ORDER_NBR AS DS_ORDER_NBR,
        DS_ORDER_SEQ_NBR AS DS_ORDER_SEQ_NBR,
        TXN_TYPE_ID AS TXN_TYPE_ID,
        TXN_WAS_MID_VOIDED_FLAG AS TXN_WAS_MID_VOIDED_FLAG,
        TXN_WAS_POST_VOIDED_FLAG AS TXN_WAS_POST_VOIDED_FLAG,
        COUNTRY_CD AS COUNTRY_CD,
        LOCATION_ID AS LOCATION_ID,
        LOAD_TSTMP AS LOAD_TSTMP,
        DS_CHANNEL AS DS_CHANNEL,
        DS_ASSIST_SITE_NBR AS DS_ASSIST_SITE_NBR,
        DS_ASSIST_LOCATION_ID AS DS_ASSIST_LOCATION_ID,
        DS_CURRENCY_CD AS DS_CURRENCY_CD,
        Monotonically_Increasing_Id AS Monotonically_Increasing_Id 
    FROM
        DW_LOAD_CONTROL_0 
    WHERE
        DW_LOAD_CONTROL_DT = date_trunc('DAY', current_timestamp)""")

df_1.createOrReplaceTempView("SQ_Shortcut_to_DW_LOAD_CONTROL1_1")

# COMMAND ----------
# DBTITLE 1, EXP_STX_TRUNC_DATE_2


df_2=spark.sql("""
    SELECT
        date_trunc('DAY',
        DW_LOAD_CONTROL_DT) AS O_DW_LOAD_CONTROL_DT,
        Monotonically_Increasing_Id AS Monotonically_Increasing_Id 
    FROM
        SQ_Shortcut_to_DW_LOAD_CONTROL1_1""")

df_2.createOrReplaceTempView("EXP_STX_TRUNC_DATE_2")

# COMMAND ----------
# DBTITLE 1, AGG_STX_COUNT_3


df_3=spark.sql("""
    SELECT
        COUNT(*) AS COUNT,
        DW_LOAD_CONTROL_DT AS DW_LOAD_CONTROL_DT,
        DS_CHANNEL AS DS_CHANNEL 
    FROM
        SQ_Shortcut_to_DW_LOAD_CONTROL1_1 
    GROUP BY
        DW_LOAD_CONTROL_DT,
        DS_CHANNEL""")

df_3.createOrReplaceTempView("AGG_STX_COUNT_3")

df_3=spark.sql("""
    SELECT
        COUNT(*) AS COUNT,
        DW_LOAD_CONTROL_DT AS DW_LOAD_CONTROL_DT,
        DS_CHANNEL AS DS_CHANNEL 
    FROM
        EXP_STX_TRUNC_DATE_2 
    GROUP BY
        DW_LOAD_CONTROL_DT,
        DS_CHANNEL""")

df_3.createOrReplaceTempView("AGG_STX_COUNT_3")

# COMMAND ----------
# DBTITLE 1, SALES_TRANS_TXN_4


df_4=spark.sql("""
    SELECT
        DAY_DT AS DAY_DT,
        LOCATION_ID AS LOCATION_ID,
        SALES_INSTANCE_ID AS SALES_INSTANCE_ID,
        SALES_TYPE_ID AS SALES_TYPE_ID,
        VOID_TYPE_CD AS VOID_TYPE_CD,
        TXN_WAS_POST_VOIDED_FLAG AS TXN_WAS_POST_VOIDED_FLAG,
        SALES_MID_VOID_REASON_CD AS SALES_MID_VOID_REASON_CD,
        CUST_TRANS_ID AS CUST_TRANS_ID,
        TRANS_TSTMP AS TRANS_TSTMP,
        TXN_END_TSTMP AS TXN_END_TSTMP,
        REGISTER_NBR AS REGISTER_NBR,
        TRANSACTION_NBR AS TRANSACTION_NBR,
        TXN_CONTROL_ID AS TXN_CONTROL_ID,
        ORDER_NBR AS ORDER_NBR,
        ORDER_SEQ_NBR AS ORDER_SEQ_NBR,
        ORDER_CHANNEL AS ORDER_CHANNEL,
        ORDER_ASSIST_LOCATION_ID AS ORDER_ASSIST_LOCATION_ID,
        ORDER_FULFILLMENT_CHANNEL AS ORDER_FULFILLMENT_CHANNEL,
        ORDER_CREATION_CHANNEL AS ORDER_CREATION_CHANNEL,
        ORDER_CREATION_DEVICE_TYPE AS ORDER_CREATION_DEVICE_TYPE,
        ORDER_CREATION_DEVICE_WIDTH AS ORDER_CREATION_DEVICE_WIDTH,
        TXN_SEGMENT AS TXN_SEGMENT,
        PAYMENT_DEVICE_TYPE AS PAYMENT_DEVICE_TYPE,
        BP_SOURCE_CD AS BP_SOURCE_CD,
        TRANS_FLAG AS TRANS_FLAG,
        SALES_CUST_CAPTURE_CD AS SALES_CUST_CAPTURE_CD,
        ZIP_CODE AS ZIP_CODE,
        EMPLOYEE_ID AS EMPLOYEE_ID,
        CASHIER_NBR AS CASHIER_NBR,
        MANAGER_NBR AS MANAGER_NBR,
        TAX_EXEMPT_ID AS TAX_EXEMPT_ID,
        COMM_TILL_FLG AS COMM_TILL_FLG,
        SPECIAL_ORD_NBR AS SPECIAL_ORD_NBR,
        PETPERK_OVERRIDE_NBR AS PETPERK_OVERRIDE_NBR,
        PETPERK_EMAIL_IND AS PETPERK_EMAIL_IND,
        PETPERK_FIRST_NAME_IND AS PETPERK_FIRST_NAME_IND,
        PETPERK_LAST_NAME_IND AS PETPERK_LAST_NAME_IND,
        PETPERK_PHONE_NBR_IND AS PETPERK_PHONE_NBR_IND,
        LOYALTY_NBR AS LOYALTY_NBR,
        LOYALTY_REDEMPTION_ID AS LOYALTY_REDEMPTION_ID,
        LUID AS LUID,
        POINTS_REDEEMED AS POINTS_REDEEMED,
        BASE_POINTS_EARNED AS BASE_POINTS_EARNED,
        BONUS_POINTS_EARNED AS BONUS_POINTS_EARNED,
        POINT_BALANCE AS POINT_BALANCE,
        POINTS_DEDUCTED AS POINTS_DEDUCTED,
        CDC_DCOL_RAW_TXT AS CDC_DCOL_RAW_TXT,
        CDC_EMAIL_ID AS CDC_EMAIL_ID,
        CDC_FIRST_NAME_ID AS CDC_FIRST_NAME_ID,
        CDC_LAST_NAME_ID AS CDC_LAST_NAME_ID,
        CDC_PHONE_NBR_ID AS CDC_PHONE_NBR_ID,
        PHONE_TYPE AS PHONE_TYPE,
        OPT_OUT_EMAIL_FLAG AS OPT_OUT_EMAIL_FLAG,
        OPT_OUT_TEXT_FLAG AS OPT_OUT_TEXT_FLAG,
        DIGITAL_RECEIPT_ANSWER_CD AS DIGITAL_RECEIPT_ANSWER_CD,
        OFFLINE_CUST_LKP_IND AS OFFLINE_CUST_LKP_IND,
        POS_OFFLINE_REASON_ID AS POS_OFFLINE_REASON_ID,
        SALES_AMT AS SALES_AMT,
        SALES_COST AS SALES_COST,
        SALES_QTY AS SALES_QTY,
        RETURN_AMT AS RETURN_AMT,
        RETURN_COST AS RETURN_COST,
        RETURN_QTY AS RETURN_QTY,
        SPECIAL_SRVC_AMT AS SPECIAL_SRVC_AMT,
        SPECIAL_SRVC_QTY AS SPECIAL_SRVC_QTY,
        NET_COUPON_AMT AS NET_COUPON_AMT,
        NET_COUPON_QTY AS NET_COUPON_QTY,
        NET_SALES_AMT AS NET_SALES_AMT,
        NET_SALES_COST AS NET_SALES_COST,
        NET_SALES_QTY AS NET_SALES_QTY,
        NET_DISC_AMT AS NET_DISC_AMT,
        NET_DISC_QTY AS NET_DISC_QTY,
        NET_MERCH_DISC_AMT AS NET_MERCH_DISC_AMT,
        NET_MERCH_DISC_QTY AS NET_MERCH_DISC_QTY,
        NET_SPECIAL_SALES_AMT AS NET_SPECIAL_SALES_AMT,
        NET_SPECIAL_SALES_QTY AS NET_SPECIAL_SALES_QTY,
        NET_SALES_TAX_AMT AS NET_SALES_TAX_AMT,
        NET_PAYMENT_AMT AS NET_PAYMENT_AMT,
        EXCH_RATE_PCT AS EXCH_RATE_PCT,
        UPDATE_TSTMP AS UPDATE_TSTMP,
        DATE_LOADED AS DATE_LOADED,
        monotonically_increasing_id() AS Monotonically_Increasing_Id 
    FROM
        SALES_TRANS_TXN""")

df_4.createOrReplaceTempView("SALES_TRANS_TXN_4")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_SALES_TRANS_TXN_5


df_5=spark.sql("""
    SELECT
        DAY_DT AS DAY_DT,
        LOCATION_ID AS LOCATION_ID,
        SALES_INSTANCE_ID AS SALES_INSTANCE_ID,
        SALES_TYPE_ID AS SALES_TYPE_ID,
        VOID_TYPE_CD AS VOID_TYPE_CD,
        TXN_WAS_POST_VOIDED_FLAG AS TXN_WAS_POST_VOIDED_FLAG,
        SALES_MID_VOID_REASON_CD AS SALES_MID_VOID_REASON_CD,
        CUST_TRANS_ID AS CUST_TRANS_ID,
        TRANS_TSTMP AS TRANS_TSTMP,
        TXN_END_TSTMP AS TXN_END_TSTMP,
        REGISTER_NBR AS REGISTER_NBR,
        TRANSACTION_NBR AS TRANSACTION_NBR,
        TXN_CONTROL_ID AS TXN_CONTROL_ID,
        ORDER_NBR AS ORDER_NBR,
        ORDER_SEQ_NBR AS ORDER_SEQ_NBR,
        ORDER_CHANNEL AS ORDER_CHANNEL,
        ORDER_ASSIST_LOCATION_ID AS ORDER_ASSIST_LOCATION_ID,
        ORDER_FULFILLMENT_CHANNEL AS ORDER_FULFILLMENT_CHANNEL,
        ORDER_CREATION_CHANNEL AS ORDER_CREATION_CHANNEL,
        ORDER_CREATION_DEVICE_TYPE AS ORDER_CREATION_DEVICE_TYPE,
        ORDER_CREATION_DEVICE_WIDTH AS ORDER_CREATION_DEVICE_WIDTH,
        TXN_SEGMENT AS TXN_SEGMENT,
        PAYMENT_DEVICE_TYPE AS PAYMENT_DEVICE_TYPE,
        BP_SOURCE_CD AS BP_SOURCE_CD,
        TRANS_FLAG AS TRANS_FLAG,
        SALES_CUST_CAPTURE_CD AS SALES_CUST_CAPTURE_CD,
        ZIP_CODE AS ZIP_CODE,
        EMPLOYEE_ID AS EMPLOYEE_ID,
        CASHIER_NBR AS CASHIER_NBR,
        MANAGER_NBR AS MANAGER_NBR,
        TAX_EXEMPT_ID AS TAX_EXEMPT_ID,
        COMM_TILL_FLG AS COMM_TILL_FLG,
        SPECIAL_ORD_NBR AS SPECIAL_ORD_NBR,
        PETPERK_OVERRIDE_NBR AS PETPERK_OVERRIDE_NBR,
        PETPERK_EMAIL_IND AS PETPERK_EMAIL_IND,
        PETPERK_FIRST_NAME_IND AS PETPERK_FIRST_NAME_IND,
        PETPERK_LAST_NAME_IND AS PETPERK_LAST_NAME_IND,
        PETPERK_PHONE_NBR_IND AS PETPERK_PHONE_NBR_IND,
        LOYALTY_NBR AS LOYALTY_NBR,
        LOYALTY_REDEMPTION_ID AS LOYALTY_REDEMPTION_ID,
        LUID AS LUID,
        POINTS_REDEEMED AS POINTS_REDEEMED,
        BASE_POINTS_EARNED AS BASE_POINTS_EARNED,
        BONUS_POINTS_EARNED AS BONUS_POINTS_EARNED,
        POINT_BALANCE AS POINT_BALANCE,
        POINTS_DEDUCTED AS POINTS_DEDUCTED,
        CDC_DCOL_RAW_TXT AS CDC_DCOL_RAW_TXT,
        CDC_EMAIL_ID AS CDC_EMAIL_ID,
        CDC_FIRST_NAME_ID AS CDC_FIRST_NAME_ID,
        CDC_LAST_NAME_ID AS CDC_LAST_NAME_ID,
        CDC_PHONE_NBR_ID AS CDC_PHONE_NBR_ID,
        PHONE_TYPE AS PHONE_TYPE,
        OPT_OUT_EMAIL_FLAG AS OPT_OUT_EMAIL_FLAG,
        OPT_OUT_TEXT_FLAG AS OPT_OUT_TEXT_FLAG,
        DIGITAL_RECEIPT_ANSWER_CD AS DIGITAL_RECEIPT_ANSWER_CD,
        OFFLINE_CUST_LKP_IND AS OFFLINE_CUST_LKP_IND,
        POS_OFFLINE_REASON_ID AS POS_OFFLINE_REASON_ID,
        SALES_AMT AS SALES_AMT,
        SALES_COST AS SALES_COST,
        SALES_QTY AS SALES_QTY,
        RETURN_AMT AS RETURN_AMT,
        RETURN_COST AS RETURN_COST,
        RETURN_QTY AS RETURN_QTY,
        SPECIAL_SRVC_AMT AS SPECIAL_SRVC_AMT,
        SPECIAL_SRVC_QTY AS SPECIAL_SRVC_QTY,
        NET_COUPON_AMT AS NET_COUPON_AMT,
        NET_COUPON_QTY AS NET_COUPON_QTY,
        NET_SALES_AMT AS NET_SALES_AMT,
        NET_SALES_COST AS NET_SALES_COST,
        NET_SALES_QTY AS NET_SALES_QTY,
        NET_DISC_AMT AS NET_DISC_AMT,
        NET_DISC_QTY AS NET_DISC_QTY,
        NET_MERCH_DISC_AMT AS NET_MERCH_DISC_AMT,
        NET_MERCH_DISC_QTY AS NET_MERCH_DISC_QTY,
        NET_SPECIAL_SALES_AMT AS NET_SPECIAL_SALES_AMT,
        NET_SPECIAL_SALES_QTY AS NET_SPECIAL_SALES_QTY,
        NET_SALES_TAX_AMT AS NET_SALES_TAX_AMT,
        NET_PAYMENT_AMT AS NET_PAYMENT_AMT,
        EXCH_RATE_PCT AS EXCH_RATE_PCT,
        UPDATE_TSTMP AS UPDATE_TSTMP,
        DATE_LOADED AS DATE_LOADED,
        Monotonically_Increasing_Id AS Monotonically_Increasing_Id 
    FROM
        SALES_TRANS_TXN_4 
    WHERE
        DATE_LOADED = CURRENT_DATE""")

df_5.createOrReplaceTempView("SQ_Shortcut_to_SALES_TRANS_TXN_5")

# COMMAND ----------
# DBTITLE 1, EXP_EDW_TRUNC_DATE_6


df_6=spark.sql("""
    SELECT
        date_trunc('DAY',
        DATE_LOADED) AS O_DATE_LOADED,
        Monotonically_Increasing_Id AS Monotonically_Increasing_Id 
    FROM
        SQ_Shortcut_to_SALES_TRANS_TXN_5""")

df_6.createOrReplaceTempView("EXP_EDW_TRUNC_DATE_6")

# COMMAND ----------
# DBTITLE 1, AGG_EDW_COUNT_7


df_7=spark.sql("""
    SELECT
        COUNT(*) AS COUNT,
        DATE_LOADED AS DATE_LOADED,
        ORDER_CHANNEL AS ORDER_CHANNEL 
    FROM
        SQ_Shortcut_to_SALES_TRANS_TXN_5 
    GROUP BY
        DATE_LOADED,
        ORDER_CHANNEL""")

df_7.createOrReplaceTempView("AGG_EDW_COUNT_7")

df_7=spark.sql("""
    SELECT
        COUNT(*) AS COUNT,
        DATE_LOADED AS DATE_LOADED,
        ORDER_CHANNEL AS ORDER_CHANNEL 
    FROM
        EXP_EDW_TRUNC_DATE_6 
    GROUP BY
        DATE_LOADED,
        ORDER_CHANNEL""")

df_7.createOrReplaceTempView("AGG_EDW_COUNT_7")

# COMMAND ----------
# DBTITLE 1, JNR_EDW_STX_8


df_8=spark.sql("""
    SELECT
        DETAIL.COUNT AS EDW_COUNT,
        DETAIL.DATE_LOADED AS EDW_DATE_LOADED,
        DETAIL.ORDER_CHANNEL AS EDW_ORDER_CHANNEL,
        MASTER.COUNT AS STX_COUNT,
        MASTER.DW_LOAD_CONTROL_DT AS STX_DW_LOAD_CONTROL_DT,
        MASTER.DS_CHANNEL AS STX_DS_CHANNEL,
        MASTER.Monotonically_Increasing_Id AS Monotonically_Increasing_Id 
    FROM
        AGG_STX_COUNT_3 MASTER 
    INNER JOIN
        AGG_EDW_COUNT_7 DETAIL 
            ON MASTER.DW_LOAD_CONTROL_DT = DETAIL.DATE_LOADED""")

df_8.createOrReplaceTempView("JNR_EDW_STX_8")

# COMMAND ----------
# DBTITLE 1, EXP_CHANNEL_CD_CALC_9


df_9=spark.sql("""
    SELECT
        EDW_DATE_LOADED AS EDW_DATE_LOADED,
        current_timestamp AS BATCH_DATE,
        iif() AS EDW_AOS,
        iif() AS EDW_ISPU,
        iif() AS EDW_SFS,
        iif() AS EDW_STR,
        iif() AS EDW_WEB,
        STX_DW_LOAD_CONTROL_DT AS STX_DW_LOAD_CONTROL_DT,
        iif() AS STX_AOS,
        iif() AS STX_ISPU,
        iif() AS STX_SFS,
        iif() AS STX_STR,
        iif() AS STX_WEB,
        Monotonically_Increasing_Id AS Monotonically_Increasing_Id 
    FROM
        JNR_EDW_STX_8""")

df_9.createOrReplaceTempView("EXP_CHANNEL_CD_CALC_9")

# COMMAND ----------
# DBTITLE 1, AGG_EDW_STX_COUNT_10


df_10=spark.sql("""
    SELECT
        BATCH_DATE AS BATCH_DATE,
        max(EDW_AOS) AS o_AOS,
        MAX(EDW_ISPU) AS o_ISPU,
        MAX(EDW_SFS) AS o_SFS,
        MAX(EDW_STR) AS o_STR,
        MAX(EDW_WEB) AS o_WEB,
        SUM(EDW_AOS + EDW_ISPU + EDW_SFS + EDW_STR + EDW_WEB) AS EDW_COUNT,
        SUM(STX_AOS + STX_ISPU + STX_SFS + STX_STR + STX_WEB) AS STX_COUNT 
    FROM
        EXP_CHANNEL_CD_CALC_9 
    GROUP BY
        BATCH_DATE""")

df_10.createOrReplaceTempView("AGG_EDW_STX_COUNT_10")

# COMMAND ----------
# DBTITLE 1, BATCH_LOAD_AUD_LOG


spark.sql("""INSERT INTO BATCH_LOAD_AUD_LOG SELECT DAY_DT AS DAY_DT,
BATCH_DATE AS BATCH_DATE,
AOS AS AOS,
ISPU AS ISPU,
SFS AS SFS,
STR AS STR,
WEB AS WEB,
STX_COUNT AS STX_COUNT,
EDW_COUNT AS EDW_COUNT,
EDW_SALES AS EDW_SALES,
STX_SALES AS STX_SALES,
PLAN_SALES_AMT AS PLAN_SALES,
ACTUAL_SALES_AMT AS ACTUAL_SALES,
SALES_VARIANCE AS SALES_VARIANCE,
PLAN_VARIANCE AS PLAN_VARIANCE FROM AGG_EDW_STX_COUNT_10""")

# COMMAND ----------
# DBTITLE 1, FLAT_DW_LOAD_CONTROL_STX


spark.sql("""INSERT INTO FLAT_DW_LOAD_CONTROL_STX SELECT COUNT AS COUNT,
DW_LOAD_CONTROL_DT AS DW_LOAD_CONTROL_DT,
DS_CHANNEL AS DS_CHANNEL FROM AGG_STX_COUNT_3""")

# COMMAND ----------
# DBTITLE 1, FLAT_SALES_TRANS_TXN_EDW


spark.sql("""INSERT INTO FLAT_SALES_TRANS_TXN_EDW SELECT COUNT AS COUNT,
DATE_LOADED AS DW_LOAD_CONTROL_DT,
ORDER_CHANNEL AS DS_CHANNEL FROM AGG_EDW_COUNT_7""")