# Databricks notebook source
# MAGIC %run "./udf_informatica"

# COMMAND ----------


from pyspark.sql.types import *

spark.sql("use DELTA_TRAINING")
spark.sql("set spark.sql.legacy.timeParserPolicy = LEGACY")


# COMMAND ----------

# DBTITLE 1, SITE_PROFILE_PRE_0

df_0=spark.sql("""
    SELECT
        LOCATION_ID AS LOCATION_ID,
        COMPANY_DESC AS COMPANY_DESC,
        COMPANY_ID AS COMPANY_ID,
        ADD_DT AS ADD_DT,
        CLOSE_DT AS CLOSE_DT,
        DELETE_DT AS DELETE_DT,
        OPEN_DT AS OPEN_DT,
        DISTRICT_DESC AS DISTRICT_DESC,
        DISTRICT_ID AS DISTRICT_ID,
        PRICE_AD_ZONE_DESC AS PRICE_AD_ZONE_DESC,
        PRICE_AD_ZONE_ID AS PRICE_AD_ZONE_ID,
        PRICE_ZONE_DESC AS PRICE_ZONE_DESC,
        PRICE_ZONE_ID AS PRICE_ZONE_ID,
        REGION_DESC AS REGION_DESC,
        REGION_ID AS REGION_ID,
        REPL_DC_NBR AS REPL_DC_NBR,
        REPL_FISH_DC_NBR AS REPL_FISH_DC_NBR,
        REPL_FWD_DC_NBR AS REPL_FWD_DC_NBR,
        COUNTRY_CD AS COUNTRY_CD,
        COUNTRY_NAME AS COUNTRY_NAME,
        STORE_NAME AS STORE_NAME,
        STORE_NBR AS STORE_NBR,
        STORE_OPEN_CLOSE_FLAG AS STORE_OPEN_CLOSE_FLAG,
        STATE_CD AS STATE_CD,
        STORE_TYPE_DESC AS STORE_TYPE_DESC,
        STORE_TYPE_ID AS STORE_TYPE_ID,
        EQUINE_MERCH_DESC AS EQUINE_MERCH_DESC,
        GR_OPEN_DT AS GR_OPEN_DT,
        SQ_FEET_RETAIL AS SQ_FEET_RETAIL,
        SQ_FEET_TOTAL AS SQ_FEET_TOTAL,
        BP_COMPANY_NBR AS BP_COMPANY_NBR,
        BP_GL_ACCT AS BP_GL_ACCT,
        SITE_ADDRESS AS SITE_ADDRESS,
        POSTAL_CD AS POSTAL_CD,
        SITE_CITY AS SITE_CITY,
        SITE_MAIN_TELE_NO AS SITE_MAIN_TELE_NO,
        SITE_GROOM_TELE_NO AS SITE_GROOM_TELE_NO,
        LOAD_DT AS LOAD_DT,
        PROMO_LABEL_CD AS PROMO_LABEL_CD,
        SFT_OPEN_DT AS SFT_OPEN_DT,
        BANFIELD_FLAG AS BANFIELD_FLAG,
        SALES_AREA_FLR_SPC AS SALES_AREA_FLR_SPC,
        SITE_CATEGORY AS SITE_CATEGORY,
        monotonically_increasing_id() AS Monotonically_Increasing_Id 
    FROM
        SITE_PROFILE_PRE""")

df_0.createOrReplaceTempView("SITE_PROFILE_PRE_0")

# COMMAND ----------

# DBTITLE 1, ASQ_SITE_PROFILE_PRE_1

df_1=spark.sql("""
    SELECT
        NVL(MAX(REGION_DESC),
        'Not Defined') REGION_DESC,
        REGION_ID 
    FROM
        SITE_PROFILE_PRE 
    GROUP BY
        REGION_ID""")

df_1.createOrReplaceTempView("ASQ_SITE_PROFILE_PRE_1")

# COMMAND ----------

# DBTITLE 1, REGION

spark.sql("""INSERT INTO REGION SELECT REGION_ID AS REGION_ID,
REGION_DESC AS REGION_DESC FROM ASQ_SITE_PROFILE_PRE_1""")
