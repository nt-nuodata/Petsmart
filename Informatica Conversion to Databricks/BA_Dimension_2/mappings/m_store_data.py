# Databricks notebook source
# MAGIC %run "./udf_informatica"

# COMMAND ----------


from pyspark.sql.types import *

spark.sql("use DELTA_TRAINING")
spark.sql("set spark.sql.legacy.timeParserPolicy = LEGACY")


# COMMAND ----------
# DBTITLE 1, STORE_DATA_FLAT_0


df_0=spark.sql("""
    SELECT
        SITE_NBR AS SITE_NBR,
        COMPANY_CD AS COMPANY_CD,
        CURRENCY_CD AS CURRENCY_CD,
        MERCH_ID AS MERCH_ID,
        TAX_JURISDICTION_CD AS TAX_JURISDICTION_CD,
        EARLIEST_VALUE_POST_DT AS EARLIEST_VALUE_POST_DT,
        TOT_VALUE_WARNING_LMT AS TOT_VALUE_WARNING_LMT,
        TOT_VALUE_ERROR_LMT AS TOT_VALUE_ERROR_LMT,
        TRANS_CNT_LOWER_LMT AS TRANS_CNT_LOWER_LMT,
        TRANS_CNT_WARNING_LMT AS TRANS_CNT_WARNING_LMT,
        TRANS_CNT_ERROR_LMT AS TRANS_CNT_ERROR_LMT,
        TRANS_VALUE_ERROR_LMT AS TRANS_VALUE_ERROR_LMT,
        TRANS_SEQ_GAP_CNT_ERROR_LMT AS TRANS_SEQ_GAP_CNT_ERROR_LMT,
        OUT_OF_BALANCE_CNT AS OUT_OF_BALANCE_CNT,
        OUT_OF_BALANCE_VALUE AS OUT_OF_BALANCE_VALUE,
        BANK_DEPOSIT_VAR_PCT AS BANK_DEPOSIT_VAR_PCT,
        CREATE_DT AS CREATE_DT,
        CREATE_USER AS CREATE_USER,
        UPDATE_DT AS UPDATE_DT,
        UPDATE_USER AS UPDATE_USER,
        monotonically_increasing_id() AS Monotonically_Increasing_Id 
    FROM
        STORE_DATA_FLAT""")

df_0.createOrReplaceTempView("STORE_DATA_FLAT_0")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_STORE_DATA_FLAT1_1


df_1=spark.sql("""
    SELECT
        SITE_NBR AS SITE_NBR,
        COMPANY_CD AS COMPANY_CD,
        CURRENCY_CD AS CURRENCY_CD,
        MERCH_ID AS MERCH_ID,
        TAX_JURISDICTION_CD AS TAX_JURISDICTION_CD,
        EARLIEST_VALUE_POST_DT AS EARLIEST_VALUE_POST_DT,
        TOT_VALUE_WARNING_LMT AS TOT_VALUE_WARNING_LMT,
        TOT_VALUE_ERROR_LMT AS TOT_VALUE_ERROR_LMT,
        TRANS_CNT_LOWER_LMT AS TRANS_CNT_LOWER_LMT,
        TRANS_CNT_WARNING_LMT AS TRANS_CNT_WARNING_LMT,
        TRANS_CNT_ERROR_LMT AS TRANS_CNT_ERROR_LMT,
        TRANS_VALUE_ERROR_LMT AS TRANS_VALUE_ERROR_LMT,
        TRANS_SEQ_GAP_CNT_ERROR_LMT AS TRANS_SEQ_GAP_CNT_ERROR_LMT,
        OUT_OF_BALANCE_CNT AS OUT_OF_BALANCE_CNT,
        OUT_OF_BALANCE_VALUE AS OUT_OF_BALANCE_VALUE,
        BANK_DEPOSIT_VAR_PCT AS BANK_DEPOSIT_VAR_PCT,
        CREATE_DT AS CREATE_DT,
        CREATE_USER AS CREATE_USER,
        UPDATE_DT AS UPDATE_DT,
        UPDATE_USER AS UPDATE_USER,
        Monotonically_Increasing_Id AS Monotonically_Increasing_Id 
    FROM
        STORE_DATA_FLAT_0""")

df_1.createOrReplaceTempView("SQ_Shortcut_to_STORE_DATA_FLAT1_1")

# COMMAND ----------
# DBTITLE 1, EXPTRANS_2


df_2=spark.sql("""
    SELECT
        SITE_NBR AS SITE_NBR,
        COMPANY_CD AS COMPANY_CD,
        CURRENCY_CD AS CURRENCY_CD,
        MERCH_ID AS MERCH_ID,
        TAX_JURISDICTION_CD AS TAX_JURISDICTION_CD,
        TO_DATE(EARLIEST_VALUE_POST_DT,
        'YYYYMMDD') AS o_EARLIEST_VALUE_POST_DT,
        TO_DECIMAL(TOT_VALUE_WARNING_LMT,
        2) AS o_TOT_VALUE_WARNING_LMT,
        TO_DECIMAL(TOT_VALUE_ERROR_LMT,
        2) AS o_TOT_VALUE_ERROR_LMT,
        TRANS_CNT_LOWER_LMT AS TRANS_CNT_LOWER_LMT,
        TRANS_CNT_WARNING_LMT AS TRANS_CNT_WARNING_LMT,
        TRANS_CNT_ERROR_LMT AS TRANS_CNT_ERROR_LMT,
        TRANS_VALUE_ERROR_LMT AS TRANS_VALUE_ERROR_LMT,
        TRANS_SEQ_GAP_CNT_ERR_LMT AS TRANS_SEQ_GAP_CNT_ERR_LMT,
        TO_DECIMAL(OUT_OF_BALANCE_CNT,
        2) AS o_OUT_OF_BALANCE_CNT,
        TO_DECIMAL(OUT_OF_BALANCE_VALUE,
        2) AS o_OUT_OF_BALANCE_VALUE,
        TO_DECIMAL(BANK_DEPOSIT_VAR_PCT,
        2) AS o_BANK_DEPOSIT_VAR_PCT,
        TO_DATE(CREATE_DT,
        'YYYYMMDD') AS o_CREATE_DT,
        CREATE_USER AS CREATE_USER,
        TO_DATE(UPDATE_DT,
        'YYYYMMDD') AS o_UPDATE_DT,
        UPDATE_USER AS UPDATE_USER,
        Monotonically_Increasing_Id AS Monotonically_Increasing_Id 
    FROM
        SQ_Shortcut_to_STORE_DATA_FLAT1_1""")

df_2.createOrReplaceTempView("EXPTRANS_2")

# COMMAND ----------
# DBTITLE 1, STORE_DATA


spark.sql("""INSERT INTO STORE_DATA SELECT SITE_NBR AS SITE_NBR,
COMPANY_CD AS COMPANY_CD,
CURRENCY_CD AS CURRENCY_CD,
MERCH_ID AS MERCH_ID,
TAX_JURISDICTION_CD AS TAX_JURISDICTION_CD,
o_EARLIEST_VALUE_POST_DT AS EARLIEST_VALUE_POST_DT,
o_TOT_VALUE_WARNING_LMT AS TOT_VALUE_WARNING_LMT,
o_TOT_VALUE_ERROR_LMT AS TOT_VALUE_ERROR_LMT,
TRANS_CNT_LOWER_LMT AS TRANS_CNT_LOWER_LMT,
TRANS_CNT_WARNING_LMT AS TRANS_CNT_WARNING_LMT,
TRANS_CNT_ERROR_LMT AS TRANS_CNT_ERR_LMT,
TRANS_VALUE_ERROR_LMT AS TRANS_VALUE_ERR_LMT,
TRANS_SEQ_GAP_CNT_ERR_LMT AS TRANS_SEQ_GAP_CNT_ERR_LMT,
o_OUT_OF_BALANCE_CNT AS OUT_OF_BAL_CNT,
o_OUT_OF_BALANCE_VALUE AS OUT_OF_BAL_VALUE,
o_BANK_DEPOSIT_VAR_PCT AS BANK_DEPOSIT_VAR_PCT,
o_CREATE_DT AS CREATE_DT,
CREATE_USER AS CREATE_USER,
o_UPDATE_DT AS UPDATE_DT,
UPDATE_USER AS UPDATE_USER FROM EXPTRANS_2""")