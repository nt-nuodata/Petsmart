# Databricks notebook source
# MAGIC %run "./udf_informatica"

# COMMAND ----------


from pyspark.sql.types import *

spark.sql("use DELTA_TRAINING")
spark.sql("set spark.sql.legacy.timeParserPolicy = LEGACY")


# COMMAND ----------
# DBTITLE 1, SERVICES_MARGIN_RATE_PRE_0


df_0=spark.sql("""
    SELECT
        WEEK_DT AS WEEK_DT,
        LOCATION_ID AS LOCATION_ID,
        SAP_DEPT_ID AS SAP_DEPT_ID,
        MARGIN_RATE AS MARGIN_RATE,
        monotonically_increasing_id() AS Monotonically_Increasing_Id 
    FROM
        SERVICES_MARGIN_RATE_PRE""")

df_0.createOrReplaceTempView("SERVICES_MARGIN_RATE_PRE_0")

# COMMAND ----------
# DBTITLE 1, SERVICES_MARGIN_RATE_1


df_1=spark.sql("""
    SELECT
        WEEK_DT AS WEEK_DT,
        LOCATION_ID AS LOCATION_ID,
        SAP_DEPT_ID AS SAP_DEPT_ID,
        MARGIN_RATE AS MARGIN_RATE,
        UPDATE_TSTMP AS UPDATE_TSTMP,
        LOAD_TSTMP AS LOAD_TSTMP,
        monotonically_increasing_id() AS Monotonically_Increasing_Id 
    FROM
        SERVICES_MARGIN_RATE""")

df_1.createOrReplaceTempView("SERVICES_MARGIN_RATE_1")

# COMMAND ----------
# DBTITLE 1, DAYS_2


df_2=spark.sql("""
    SELECT
        DAY_DT AS DAY_DT,
        BUSINESS_DAY_FLAG AS BUSINESS_DAY_FLAG,
        HOLIDAY_FLAG AS HOLIDAY_FLAG,
        DAY_OF_WK_NAME AS DAY_OF_WK_NAME,
        DAY_OF_WK_NAME_ABBR AS DAY_OF_WK_NAME_ABBR,
        DAY_OF_WK_NBR AS DAY_OF_WK_NBR,
        CAL_DAY_OF_MO_NBR AS CAL_DAY_OF_MO_NBR,
        CAL_DAY_OF_YR_NBR AS CAL_DAY_OF_YR_NBR,
        CAL_WK AS CAL_WK,
        CAL_WK_NBR AS CAL_WK_NBR,
        CAL_MO AS CAL_MO,
        CAL_MO_NBR AS CAL_MO_NBR,
        CAL_MO_NAME AS CAL_MO_NAME,
        CAL_MO_NAME_ABBR AS CAL_MO_NAME_ABBR,
        CAL_QTR AS CAL_QTR,
        CAL_QTR_NBR AS CAL_QTR_NBR,
        CAL_HALF AS CAL_HALF,
        CAL_YR AS CAL_YR,
        FISCAL_DAY_OF_MO_NBR AS FISCAL_DAY_OF_MO_NBR,
        FISCAL_DAY_OF_YR_NBR AS FISCAL_DAY_OF_YR_NBR,
        FISCAL_WK AS FISCAL_WK,
        FISCAL_WK_NBR AS FISCAL_WK_NBR,
        FISCAL_MO AS FISCAL_MO,
        FISCAL_MO_NBR AS FISCAL_MO_NBR,
        FISCAL_MO_NAME AS FISCAL_MO_NAME,
        FISCAL_MO_NAME_ABBR AS FISCAL_MO_NAME_ABBR,
        FISCAL_QTR AS FISCAL_QTR,
        FISCAL_QTR_NBR AS FISCAL_QTR_NBR,
        FISCAL_HALF AS FISCAL_HALF,
        FISCAL_YR AS FISCAL_YR,
        LYR_WEEK_DT AS LYR_WEEK_DT,
        LWK_WEEK_DT AS LWK_WEEK_DT,
        WEEK_DT AS WEEK_DT,
        EST_TIME_CONV_AMT AS EST_TIME_CONV_AMT,
        EST_TIME_CONV_HRS AS EST_TIME_CONV_HRS,
        ES0_TIME_CONV_AMT AS ES0_TIME_CONV_AMT,
        ES0_TIME_CONV_HRS AS ES0_TIME_CONV_HRS,
        CST_TIME_CONV_AMT AS CST_TIME_CONV_AMT,
        CST_TIME_CONV_HRS AS CST_TIME_CONV_HRS,
        CS0_TIME_CONV_AMT AS CS0_TIME_CONV_AMT,
        CS0_TIME_CONV_HRS AS CS0_TIME_CONV_HRS,
        MST_TIME_CONV_AMT AS MST_TIME_CONV_AMT,
        MST_TIME_CONV_HRS AS MST_TIME_CONV_HRS,
        MS0_TIME_CONV_AMT AS MS0_TIME_CONV_AMT,
        MS0_TIME_CONV_HRS AS MS0_TIME_CONV_HRS,
        PST_TIME_CONV_AMT AS PST_TIME_CONV_AMT,
        PST_TIME_CONV_HRS AS PST_TIME_CONV_HRS,
        monotonically_increasing_id() AS Monotonically_Increasing_Id 
    FROM
        DAYS""")

df_2.createOrReplaceTempView("DAYS_2")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_SERVICES_MARGIN_RATE_3


df_3=spark.sql("""WITH LATEST_RATE AS(
SELECT P.LOCATION_ID,
       P.SAP_DEPT_ID,
       CASE WHEN P.MARGIN_RATE > 1.0  
            THEN 1.0
            WHEN P.MARGIN_RATE < -1.0  
            THEN -1.0
            ELSE P.MARGIN_RATE
        END MARGIN_RATE
  FROM (
       SELECT LOCATION_ID, 
              SAP_DEPT_ID,
              MAX(WEEK_DT) AS LATEST_WEEK_DT
         FROM SERVICES_MARGIN_RATE
        WHERE WEEK_DT <= (SELECT MAX(WEEK_DT) FROM SERVICES_MARGIN_RATE_PRE)
        GROUP BY LOCATION_ID, 
              SAP_DEPT_ID
       ) S,
       SERVICES_MARGIN_RATE P
  WHERE S.LOCATION_ID    = P.LOCATION_ID
    AND S.SAP_DEPT_ID    = P.SAP_DEPT_ID
    AND S.LATEST_WEEK_DT = P.WEEK_DT
)
    
SELECT M.WEEK_DT,
       M.LOCATION_ID,
       M.SAP_DEPT_ID,
       NVL(L.MARGIN_RATE,0.0000) AS MARGIN_RATE,
       M.LOAD_TSTMP,
       'U' as LOAD_FLAG
  FROM SERVICES_MARGIN_RATE M
  LEFT JOIN LATEST_RATE L
    ON L.LOCATION_ID = M.LOCATION_ID
   AND L.SAP_DEPT_ID = M.SAP_DEPT_ID
 WHERE WEEK_DT > (SELECT MAX(WEEK_DT) FROM SERVICES_MARGIN_RATE_PRE)
   AND M.MARGIN_RATE <> NVL(L.MARGIN_RATE,0.0000)
 
UNION ALL

SELECT TT.WEEK_DT,
       TT.LOCATION_ID,
       TT.SAP_DEPT_ID,
       TT.MARGIN_RATE,
       CURRENT_TIMESTAMP AS LOAD_TSTMP,
       'I' as LOAD_FLAG
  FROM (
       SELECT W.WEEK_DT,
              L.LOCATION_ID,
              L.SAP_DEPT_ID,
              L.MARGIN_RATE
         FROM (
              SELECT DISTINCT WEEK_DT
                FROM DAYS 
               WHERE DAY_DT BETWEEN (SELECT CAST(MAX(WEEK_DT) AS DATE) + 1 FROM SERVICES_MARGIN_RATE_PRE) 
                 AND CURRENT_DATE + 7
              ) W,
              LATEST_RATE L
       ) TT
  LEFT JOIN SERVICES_MARGIN_RATE M
    ON TT.WEEK_DT     = M.WEEK_DT
   AND TT.LOCATION_ID = M.LOCATION_ID
   AND TT.SAP_DEPT_ID = M.SAP_DEPT_ID
 WHERE M.WEEK_DT IS NULL""")

df_3.createOrReplaceTempView("SQ_Shortcut_to_SERVICES_MARGIN_RATE_3")

# COMMAND ----------
# DBTITLE 1, EXP_ESTIMATE_4


df_4=spark.sql("""
    SELECT
        WEEK_DT AS WEEK_DT,
        LOCATION_ID AS LOCATION_ID,
        SAP_DEPT_ID AS SAP_DEPT_ID,
        MARGIN_RATE AS MARGIN_RATE,
        current_timestamp AS UPDATE_TSTMP,
        IFF(LOAD_FLAG = 'I',
        current_timestamp,
        LOAD_TSTMP) AS LOAD_TSTMP,
        LOAD_FLAG AS LOAD_FLAG,
        Monotonically_Increasing_Id AS Monotonically_Increasing_Id 
    FROM
        SQ_Shortcut_to_SERVICES_MARGIN_RATE_3""")

df_4.createOrReplaceTempView("EXP_ESTIMATE_4")

# COMMAND ----------
# DBTITLE 1, UPD_STRATEGY_5


df_5=spark.sql("""
    SELECT
        WEEK_DT AS WEEK_DT,
        LOCATION_ID AS LOCATION_ID,
        SAP_DEPT_ID AS SAP_DEPT_ID,
        MARGIN_RATE AS MARGIN_RATE,
        UPDATE_TSTMP AS UPDATE_TSTMP,
        LOAD_TSTMP AS LOAD_TSTMP,
        LOAD_FLAG AS LOAD_FLAG,
        Monotonically_Increasing_Id AS Monotonically_Increasing_Id 
    FROM
        EXP_ESTIMATE_4""")

df_5.createOrReplaceTempView("UPD_STRATEGY_5")

# COMMAND ----------
# DBTITLE 1, SERVICES_MARGIN_RATE


spark.sql("""INSERT INTO SERVICES_MARGIN_RATE SELECT WEEK_DT AS WEEK_DT,
LOCATION_ID AS LOCATION_ID,
SAP_DEPT_ID AS SAP_DEPT_ID,
MARGIN_RATE AS MARGIN_RATE,
UPDATE_TSTMP AS UPDATE_TSTMP,
LOAD_TSTMP AS LOAD_TSTMP FROM UPD_STRATEGY_5""")